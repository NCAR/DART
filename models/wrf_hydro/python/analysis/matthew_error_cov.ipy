import xarray as xa
import numpy as np
import pandas as pd
import os as os
from plotnine import *
os.chdir('python')
from get_nwm_chrtout_postgres import *

#######################################################
# Setup the structure of the result.
# Goal: a data frame representing the upper triangular matrix
#       with error correlation between all pairs of gages for
#       each variable of interest.
# columns: from, to, distance, streamflow_err_cor, qbucket_err_cor, qsfclatrunoff_err_cor

## Get the gages and coordinates. from the RouteLink file
rl_file='/Users/jamesmcc/Downloads/Matthew_DOMAIN/RouteLink.nc'
rl_df=xa.open_dataset(rl_file).to_dataframe()
rl=rl_df.loc[rl_df.gages != b'               ', ['gages','x','y']]
rl['gages']=rl['gages'].str.decode("utf-8")
site_nos_list=rl['gages'].tolist()

## Create a dummy NxN pandas data frame of site names with null data
## **** remove subsetting in next line ****
dists=rl.reset_index().loc[0:,['gages']]
dists['dist']=0.0
dists = dists.set_index('gages')

## solution here
## https://stackoverflow.com/questions/39240286/create-matrix-structure-using-pandas
dists2 = pd.DataFrame(np.outer(dists, dists), dists.index, dists.index)
## Just keep the upper triangular matrix in pandas
keep = np.triu(np.ones(dists2.shape)).astype('bool').reshape(dists2.size)
dists3=dists2.stack()[keep].to_frame()
dists3.index.names=['from','to']
dists3=dists3.reset_index() 
dists3['streamflow_err_cor']     = 0.0
dists3['qbucket_err_cor']       = 0.0
dists3['qscflatrunoff_err_cor'] =0.0
dists3.columns = ['to', 'from', 'distance','streamflow_err_cor',
                  'qbucket_err_cor', 'qscflatrunoff_err_cor']


#######################################################
# NWM data section
## Get the data.
data=get_nwm_chrtout('v1_2_1', 'short_range', site_nos_list)

## Calculate the gage-wise differences with the mean / anomalies
def mk_error(vec):
    return vec - vec.mean()
data['qsfclatrunoff'] = \
    data.groupby(['site_no'])['qsfclatrunoff'].transform(mk_error)
data['qbucket'] = \
    data.groupby(['site_no'])['qbucket'].transform(mk_error)
data['streamflow_mod'] = \
    data.groupby(['site_no'])['streamflow_mod'].transform(mk_error)


#######################################################
# Fill in the "upper-triangular" correlations for each variable
# TODO: What are the assumptions to turn this into a function?
# dists3 and data?

for ii in range(0,dists3.shape[0]):
    print(str(ii) + ' / ' + str(dists3.shape[0]) )
    to_gage=dists3.iloc[ii]['to']
    from_gage=dists3.iloc[ii]['from']

    ## if on the diagonal, results are predetermined.
    if to_gage == from_gage:

        dists3.loc[ii,'distance']              = 0.0
        dists3.loc[ii,'streamflow_err_cor']    = 1.0
        dists3.loc[ii,'qbucket_err_cor']       = 1.0
        dists3.loc[ii,'qscflatrunoff_err_cor'] = 1.0

    else :
       
        to_xy  =rl[rl.gages==to_gage][['x','y']].as_matrix()
        from_xy=rl[rl.gages==from_gage][['x','y']].as_matrix()
        dists3.loc[ii,'distance'] = (( to_xy - from_xy )**2).sum()**(.5)

        to_data   = data.loc[data.site_no == to_gage.strip(), \
                             ['inittime', 'validtime',
                              'streamflow_mod', 'qbucket', 'qsfclatrunoff']]
        from_data = data.loc[data.site_no == from_gage.strip(), \
                             ['inittime', 'validtime',
                              'streamflow_mod', 'qbucket', 'qsfclatrunoff']]
        to_data.drop_duplicates(('inittime','validtime'), inplace=True)
        from_data.drop_duplicates(('inittime','validtime'), inplace=True)
        
        join_data = pd.merge(to_data, from_data, \
                             on=('inittime','validtime'), \
                             how='inner')       

        dists3.loc[ii,'streamflow_err_cor']    = \
            join_data['streamflow_mod_x'].corr(join_data['streamflow_mod_y'])
        dists3.loc[ii,'qbucket_err_cor']       = \
            join_data['qbucket_x'].corr(join_data['qbucket_y'])
        dists3.loc[ii,'qscflatrunoff_err_cor'] = \
            join_data['qsfclatrunoff_x'].corr(join_data['qsfclatrunoff_y'])


#dists3.to_pickle('matthew_error_covar.pkl')
dists3=pd.read_pickle('matthew_error_covar.pkl')
dists3['distance']=(dists3['distance']**.5)/1000

plot_filename_var = { 'matthew_streamflow_err_cor':'streamflow_err_cor', \
                      'matthew_qbucket_err_cor':'qbucket_err_cor', \
                      'matthew_qsfclatrunoff_err_cor':'qscflatrunoff_err_cor'   }

for pp in plot_filename_var:
    plot = ggplot(dists3, aes(x='distance',y=plot_filename_var[pp])) + \
                  geom_point(alpha=.2) + \
                  theme_bw() + \
                  scale_y_continuous(name='distance (km)')
    plot.save(pp+'.png','png', width=11, height=8)
